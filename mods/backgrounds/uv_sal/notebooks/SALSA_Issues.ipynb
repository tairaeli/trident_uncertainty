{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SALSA Clump UV Background Clump-Labeling Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/_collections_abc.py:666: MatplotlibDeprecationWarning: The global colormaps dictionary is no longer considered public API.\n",
      "  self[key]\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "from mpi4py import MPI\n",
    "import yt\n",
    "import trident\n",
    "import salsa\n",
    "from salsa.utils import check_rays\n",
    "import scipy.integrate as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SALSA Pipeline\n",
    "\n",
    "Here, I give a breif overview as to how we obtain our SALSA data using pre-generated ray data.\n",
    "\n",
    "To begin with, we start by loading in the halo data from Foggie that we would be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yt : [ERROR    ] 2023-09-07 12:37:12,710 None of the arguments provided to load() is a valid file\n",
      "yt : [ERROR    ] 2023-09-07 12:37:12,711 Please check that you have used a correct path\n"
     ]
    },
    {
     "ename": "YTOutputNotIdentified",
     "evalue": "Supplied ('some file path',) {}, but could not load!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mYTOutputNotIdentified\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d3dc052f776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhalo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"some file path\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhalo_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/yt/convenience.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mmylog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None of the arguments provided to load() is a valid file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mmylog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please check that you have used a correct path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mYTOutputNotIdentified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypes_to_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mYTOutputNotIdentified\u001b[0m: Supplied ('some file path',) {}, but could not load!"
     ]
    }
   ],
   "source": [
    "halo_path = \"some file path\"\n",
    "\n",
    "halo_data = yt.load(halo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I create the rays that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = halo_data.arr[,'kpc']\n",
    "\n",
    "nrays = 1\n",
    "\n",
    "max_impact = \n",
    "\n",
    "gal_vel = \n",
    "\n",
    "other_fields = [(\"density\",\"g/cm**3\"), \"temperature\", (\"metallicity\",\"Zsun\"), \"radius\"]\n",
    "\n",
    "out_path = \"./\"\n",
    "\n",
    "\n",
    "salsa.generate_lrays(halo_data, \n",
    "                     center.to('code_length'), \n",
    "                     nrays, \n",
    "                     max_impact, \n",
    "                     length=600, \n",
    "                     field_parameters={'bulk_velocity':gal_vel}, \n",
    "                     ion_list=['H I'], \n",
    "                     fields=other_fields, \n",
    "                     out_dir=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been loaded in, and the rays have been created, we can set our desired UV Background. In this code, I use the UV Background from Puchwein et al. 2019, but the same process can be followed with the other UVB that is analyzed from Haart & Madau 2012. All that needs to be changed is the respective file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray = 0\n",
    "ion = \"O VI\"\n",
    "atom,istate = ion.split(\" \")\n",
    "\n",
    "field_name = f\"{atom}_p{trident.from_roman(istate)-1}_number_density\"\n",
    "\n",
    "abuns = dict(zip(elements, abundances[1]))\n",
    "\n",
    "# uvb = \"/mnt/home/tairaeli/trident_uncertainty/mods/abundances/data_bin/hm2012_ss_hr.h5\"\n",
    "uvb = \"/mnt/home/tairaeli/trident_uncertainty/mods/abundances/data_bin/par_test.h5\"\n",
    "# uvb_name = \"HM_2012\"\n",
    "uvb_name = \"PCW_2019\"\n",
    "\n",
    "ray_dat = yt.load(f\"/mnt/scratch/tairaeli/halo2392_pcw_2019/redshift2.0/rays/ray{ray}.h5\")\n",
    "\n",
    "trident.add_ion_number_density_field(atom, trident.from_roman(istate), \n",
    "                                     ray_dat, abundance_dict = abuns, \n",
    "                                     ionization_table = uvb)\n",
    "\n",
    "pcw_dens = ray_dat.r[(\"gas\",field_name)].copy()\n",
    "\n",
    "ray_len_cm = ray_dat.domain_width.to(\"cm\")[0]\n",
    "n_cells = len(dens)\n",
    "\n",
    "ray_pos = np.linspace(0,ray_len_cm,n_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity (as well as avoiding a strange quirk with yt when it comes to setting UVBs), the results of the Haart and Madau UVB has already been saved to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./hm_dens.pickle\", \"rb\") as dens_dat:\n",
    "    hm_dens = pickle.load(dens_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given some abudnance table, we use SALSA to identify where the gas clumps are within our ray for each UVB (also showing this process for only the Puchwein UVB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abundances = abun.iloc[0].to_dict()\n",
    "            \n",
    "abs_ext = salsa.AbsorberExtractor(ds, \n",
    "                                  ray_file, \n",
    "                                  ion_name = ion, \n",
    "                                  velocity_res =20, \n",
    "                                  abundance_table = abundances, \n",
    "                                  calc_missing=True)\n",
    "\n",
    "pcw_clump_dat = salsa.get_absorbers(abs_ext, \n",
    "                         my_rays, \n",
    "                         method='spice', \n",
    "                         fields=other_fields, \n",
    "                         units_dict=field_units)\n",
    "\n",
    "pcw_clump_dat = pcw_clump_dat.drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the output data for both UV Backgrounds have been saved to pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./salsa_out_hm\", \"rb\") as salsa_dat:\n",
    "    hm_clump_dat = pickle.load(salsa_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Results\n",
    "\n",
    "Now, we create a plot of the densities for a particular ion as a function of the position along the ray. We also fill in regions where SALSA flagged a region along the ray as a \"clump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.cool([0.3,0.4,0.9,1])\n",
    "\n",
    "plt.figure(figsize = [15,8], dpi = 500, facecolor = \"white\")\n",
    "plt.semilogy(ray_pos, hm_dens,label = \"HM 2012\", color = colors[3])\n",
    "plt.semilogy(ray_pos, pcw_dens,label = \"PCW 2019\", color = colors[1])\n",
    "# plt.axhline(1e-13)\n",
    "\n",
    "hm_clumps = hm_clump_dat[ion][\"HM_2012\"][hm_dat[ion][\"HM_2012\"][\"lightray_index\"] == str(ray)]\n",
    "pcw_clumps = pcw_clump_dat[ion][\"PCW_2019\"][pcw_dat[ion][\"PCW_2019\"][\"lightray_index\"] == str(ray)]\n",
    "\n",
    "i = 0\n",
    "for i in range(len(hm_clumps[\"interval_start\"])):\n",
    "    \n",
    "    lb = hm_clumps[\"interval_start\"][i]\n",
    "    hb = hm_clumps[\"interval_end\"][i]\n",
    "    \n",
    "    rng = [lb,hb]\n",
    "    \n",
    "    yb = hm_dens[slice(*rng)]\n",
    "    xb = np.arange(*rng)\n",
    "    \n",
    "    plt.fill_between(xb,yb, color = colors[2], alpha = 0.3)\n",
    "\n",
    "for i in range(len(pcw_clumps[\"interval_start\"])):\n",
    "    \n",
    "    lb = pcw_clumps[\"interval_start\"][i]\n",
    "    hb = pcw_clumps[\"interval_end\"][i]\n",
    "    \n",
    "    rng = [lb,hb]\n",
    "    \n",
    "    yb = dens[slice(*rng)]\n",
    "    xb = np.arange(*rng)\n",
    "    \n",
    "    plt.fill_between(xb,yb, color = colors[0], alpha = 0.3)    \n",
    "\n",
    "plt.grid()\n",
    "plt.legend(fontsize = 20)\n",
    "# plt.ylim(10**(-15),10**(-6.5))\n",
    "# plt.xlim(500,900)\n",
    "plt.xlabel(\"Position Along Ray\", fontsize = 25)\n",
    "plt.ylabel(r\"Density ($cm^{-3}$)\", fontsize = 25)\n",
    "plt.title(f\"UVB Number Density Comparison\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the purple regions represent where SALSA found a clump in both UVBs and the pink and blue regions represent where SALSA only recognized a clump in 1 of these regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighting Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
