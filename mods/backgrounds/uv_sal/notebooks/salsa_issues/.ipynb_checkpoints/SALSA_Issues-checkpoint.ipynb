{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SALSA Clump UV Background Clump-Labeling Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/_collections_abc.py:666: MatplotlibDeprecationWarning: The global colormaps dictionary is no longer considered public API.\n",
      "  self[key]\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "from mpi4py import MPI\n",
    "import yt\n",
    "import trident\n",
    "import salsa\n",
    "from salsa.utils import check_rays\n",
    "import scipy.integrate as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SALSA Pipeline\n",
    "\n",
    "Here, I give a breif overview as to how we obtain our SALSA data using pre-generated ray data.\n",
    "\n",
    "To begin with, we start by loading in the halo data from FOGGIE that we would be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halo_path = \"/mnt/research/galaxies-REU/sims/FOGGIE\"\n",
    "\n",
    "halo_data = yt.load(halo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I create the rays that we will be using. To do so, we must first do a bit of preprocessing on the FOGGIE data to put the data into a more readable format. Additionally, this will also require the use of a local instalation of FOGGIE to access the information on the halo that we are looking at. This can be done by editing the following block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL \n",
    "foggie_dir = \"/mnt/home/tairaeli/astro_libs/foggie/foggie/halo_infos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above step is completed, the following cell should run without any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying which halo we are looking at and at what redshift\n",
    "halo = 2392\n",
    "rs = 20\n",
    "\n",
    "center_dat = {}\n",
    "\n",
    "# loading in FOGGIE data\n",
    "raw_foggie_dat = pd.read_csv(f\"{foggie_dir}/00{halo}/nref11c_nref9f/halo_c_v\", sep = '|', \n",
    "                             names = ['null','redshift','name','xc','yc','zc','xv','yv','zv','null2'])\n",
    "\n",
    "# making some fixes specific to these files\n",
    "raw_foggie_dat = raw_foggie_dat.drop(0)\n",
    "raw_foggie_dat = raw_foggie_dat.drop(columns = ['null','null2'])\n",
    "\n",
    "# isolating data to a specific redshift \n",
    "raw_foggie_dat = raw_foggie_dat[raw_foggie_dat['name'] == ' RD00'+str(rs)+' ']\n",
    "\n",
    "# storing the position and velocity data of the galactic center\n",
    "center_dat['pos'] = [float(raw_foggie_dat[\"xc\"]),float(raw_foggie_dat[\"yc\"]),float(raw_foggie_dat[\"zc\"])]\n",
    "center_dat['vel'] = [float(raw_foggie_dat[\"xv\"]),float(raw_foggie_dat[\"yv\"]),float(raw_foggie_dat[\"zv\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = halo_data.arr[,'kpc']\n",
    "\n",
    "nrays = 1\n",
    "\n",
    "max_impact = \n",
    "\n",
    "gal_vel = \n",
    "\n",
    "other_fields = [(\"density\",\"g/cm**3\"), \"temperature\", (\"metallicity\",\"Zsun\"), \"radius\"]\n",
    "\n",
    "out_path = \"./\"\n",
    "\n",
    "\n",
    "salsa.generate_lrays(halo_data, \n",
    "                     center.to('code_length'), \n",
    "                     nrays, \n",
    "                     max_impact, \n",
    "                     length=600, \n",
    "                     field_parameters={'bulk_velocity':gal_vel}, \n",
    "                     ion_list=['H I'], \n",
    "                     fields=other_fields, \n",
    "                     out_dir=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been loaded in, and the rays have been created, we can set our desired UV Background. In this code, I use the UV Background from Puchwein et al. 2019, but the same process can be followed with the other UVB that is analyzed from Haart & Madau 2012. All that needs to be changed is the respective file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray = 0\n",
    "ion = \"O VI\"\n",
    "atom,istate = ion.split(\" \")\n",
    "\n",
    "field_name = f\"{atom}_p{trident.from_roman(istate)-1}_number_density\"\n",
    "\n",
    "abuns = dict(zip(elements, abundances[1]))\n",
    "\n",
    "# uvb = \"/mnt/home/tairaeli/trident_uncertainty/mods/abundances/data_bin/hm2012_ss_hr.h5\"\n",
    "uvb = \"/mnt/home/tairaeli/trident_uncertainty/mods/abundances/data_bin/par_test.h5\"\n",
    "# uvb_name = \"HM_2012\"\n",
    "uvb_name = \"PCW_2019\"\n",
    "\n",
    "ray_dat = yt.load(f\"/mnt/scratch/tairaeli/halo2392_pcw_2019/redshift2.0/rays/ray{ray}.h5\")\n",
    "\n",
    "trident.add_ion_number_density_field(atom, trident.from_roman(istate), \n",
    "                                     ray_dat, abundance_dict = abuns, \n",
    "                                     ionization_table = uvb)\n",
    "\n",
    "pcw_dens = ray_dat.r[(\"gas\",field_name)].copy()\n",
    "\n",
    "ray_len_cm = ray_dat.domain_width.to(\"cm\")[0]\n",
    "n_cells = len(dens)\n",
    "\n",
    "ray_pos = np.linspace(0,ray_len_cm,n_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity (as well as avoiding a strange quirk with yt when it comes to setting UVBs), the results of the Haart and Madau UVB has already been saved to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./hm_dens.pickle\", \"rb\") as dens_dat:\n",
    "    hm_dens = pickle.load(dens_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given some abudnance table, we use SALSA to identify where the gas clumps are within our ray for each UVB (also showing this process for only the Puchwein UVB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abundances = abun.iloc[0].to_dict()\n",
    "            \n",
    "abs_ext = salsa.AbsorberExtractor(ds, \n",
    "                                  ray_file, \n",
    "                                  ion_name = ion, \n",
    "                                  velocity_res =20, \n",
    "                                  abundance_table = abundances, \n",
    "                                  calc_missing=True)\n",
    "\n",
    "pcw_clump_dat = salsa.get_absorbers(abs_ext, \n",
    "                         my_rays, \n",
    "                         method='spice', \n",
    "                         fields=other_fields, \n",
    "                         units_dict=field_units)\n",
    "\n",
    "pcw_clump_dat = pcw_clump_dat.drop(columns='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the output data for both UV Backgrounds have been saved to pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./salsa_out_hm\", \"rb\") as salsa_dat:\n",
    "    hm_clump_dat = pickle.load(salsa_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Results\n",
    "\n",
    "Now, we create a plot of the densities for a particular ion as a function of the position along the ray. We also fill in regions where SALSA flagged a region along the ray as a \"clump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.cool([0.3,0.4,0.9,1])\n",
    "\n",
    "plt.figure(figsize = [15,8], dpi = 500, facecolor = \"white\")\n",
    "plt.semilogy(ray_pos, hm_dens,label = \"HM 2012\", color = colors[3])\n",
    "plt.semilogy(ray_pos, pcw_dens,label = \"PCW 2019\", color = colors[1])\n",
    "# plt.axhline(1e-13)\n",
    "\n",
    "hm_clumps = hm_clump_dat[ion][\"HM_2012\"][hm_dat[ion][\"HM_2012\"][\"lightray_index\"] == str(ray)]\n",
    "pcw_clumps = pcw_clump_dat[ion][\"PCW_2019\"][pcw_dat[ion][\"PCW_2019\"][\"lightray_index\"] == str(ray)]\n",
    "\n",
    "i = 0\n",
    "for i in range(len(hm_clumps[\"interval_start\"])):\n",
    "    \n",
    "    lb = hm_clumps[\"interval_start\"][i]\n",
    "    hb = hm_clumps[\"interval_end\"][i]\n",
    "    \n",
    "    rng = [lb,hb]\n",
    "    \n",
    "    yb = hm_dens[slice(*rng)]\n",
    "    xb = np.arange(*rng)\n",
    "    \n",
    "    plt.fill_between(xb,yb, color = colors[2], alpha = 0.3)\n",
    "\n",
    "for i in range(len(pcw_clumps[\"interval_start\"])):\n",
    "    \n",
    "    lb = pcw_clumps[\"interval_start\"][i]\n",
    "    hb = pcw_clumps[\"interval_end\"][i]\n",
    "    \n",
    "    rng = [lb,hb]\n",
    "    \n",
    "    yb = dens[slice(*rng)]\n",
    "    xb = np.arange(*rng)\n",
    "    \n",
    "    plt.fill_between(xb,yb, color = colors[0], alpha = 0.3)    \n",
    "\n",
    "plt.grid()\n",
    "plt.legend(fontsize = 20)\n",
    "# plt.ylim(10**(-15),10**(-6.5))\n",
    "# plt.xlim(500,900)\n",
    "plt.xlabel(\"Position Along Ray\", fontsize = 25)\n",
    "plt.ylabel(r\"Density ($cm^{-3}$)\", fontsize = 25)\n",
    "plt.title(f\"UVB Number Density Comparison\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the purple regions represent where SALSA found a clump in both UVBs and the pink and blue regions represent where SALSA only recognized a clump in 1 of these regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighting Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
